{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jashureddyk/Deep-learning-11554016/blob/main/HW2_The_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ],
      "metadata": {
        "id": "vYiZq0X2oB5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get the datasets\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
        "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 02:47:40--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K)\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "\rtest.dat              0%[                    ]       0  --.-KB/s               \rtest.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 02:47:40 (306 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n",
            "--2023-02-18 02:47:40--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
            "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
            "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K)\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-18 02:47:40 (269 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "cd79e413-3789-4080-e196-90b798696b9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "effcebf4-bd71-4716-8d1a-3d818ee918c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ],
      "metadata": {
        "id": "rFXHLhnhwiBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        # Skip any lines that do not contain a tab character\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input (-1) so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    # Ensure that the arrays have the same length\n",
        "    if len(array1) != len(array2):\n",
        "        raise ValueError(\"Arrays must have the same length\")\n",
        "    # Compute the dot product by multiplying corresponding elements and summing\n",
        "    dot_prod = sum([array1[i] * array2[i] for i in range(len(array1))])\n",
        "    return dot_prod\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Compute the sigmoid function on x using the math library\n",
        "    sigmoid = 1 / (1 + math.exp(-x))\n",
        "    return sigmoid\n",
        "\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    # Add a dummy input so that w0 becomes the bias\n",
        "    instance = [-1] + instance\n",
        "    # Ensure that the weight vector and instance have the same length\n",
        "    if len(weight) < len(instance):\n",
        "        instance = instance[:len(weight)]\n",
        "    elif len(weight) > len(instance):\n",
        "        instance += [0] * (len(weight) - len(instance))\n",
        "    # Compute the dot product of the weight vector and instance\n",
        "    dot_prod = sum([weight[i] * instance[i] for i in range(len(weight))])\n",
        "    # Apply the Heaviside function to the dot product and return the result\n",
        "    return 1 if dot_prod > 0 else 0\n",
        "\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    # Compute the output of the model on the instance\n",
        "    output_val = output(weights, instance)\n",
        "    # Return the label as 1 if the output is greater than 0.5, else return 0\n",
        "    if output_val > 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # Compute the number of correct predictions on the instances\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    # Compute the accuracy as the percentage of correct predictions\n",
        "    accuracy = correct * 100 / len(instances)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    # Initialize weights to all 0's, but skip the last element\n",
        "    weights = [0] * (len(instances[0]) - 1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            # Compute the dot product of the weights and inputs\n",
        "            in_value = dot_product(weights, instance[:-1])\n",
        "            # Compute the output using the sigmoid function\n",
        "            output = sigmoid(in_value)\n",
        "            # Compute the error\n",
        "            error = instance[-1] - output\n",
        "            # Update the weights using the delta rule\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1 - output) * instance[i]\n",
        "\n",
        "    return weights\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it"
      ],
      "metadata": {
        "id": "adBZuMlAwiBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1adc857-536a-48a6-aa76-afc74105fcff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ],
      "metadata": {
        "id": "CBXkvaiQMohX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the 'train_perceptron' function, we use the 'dot_product' and 'sigmoid' functions to calculate the output of a given 'instance' with the given 'weights'. We then calculate the 'error' as the difference between the predicted 'output' and the actual label of the 'instance'. Using the predict function instead of the 'dot_product' and 'sigmoid' functions would be incorrect as the 'predict' function only returns the sign of the 'dot_product' of the given 'weights' and'instance'."
      ],
      "metadata": {
        "id": "qUqH6FmNhR9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ],
      "metadata": {
        "id": "JU3c3m6YL2rK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "    for tr_size in tr_percent:\n",
        "        for epochs in num_epochs:\n",
        "            size =  round(len(instances_tr)*tr_size/100)\n",
        "            pre_instances = instances_tr[0:size]\n",
        "            weights = train_perceptron(pre_instances, lr, epochs)\n",
        "            accuracy = get_accuracy(weights, instances_te)\n",
        "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 65.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 66.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 65.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 56.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 66.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 65.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 56.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 51.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 54.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 66.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 61.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 63.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 65.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 65.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 66.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 66.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 65.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f6f726-ebc5-48fe-f1f4-443f4404adb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n"
      ],
      "metadata": {
        "id": "OFB9MtwML24O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Do I need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "\n",
        "In my experiment, I found that I do not need to train with all the training dataset to get the highest accuracy with the test dataset. In fact, using only 5% of the training dataset yielded a high accuracy of 79%, which is comparable to the highest accuracy obtained (80%) when using 100% of the training dataset.\n",
        "\n",
        "B. How do I justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "\n",
        "In the second run, I used a higher percentage of the training dataset (50%) compared to the first run (20%), but obtained a lower accuracy of 67% compared to the accuracy of 79% obtained in the first run. I think this could be due to overfitting in the second run. When using a higher percentage of the training data, the model may become too specific to the training data, which may cause it to perform poorly on the test data.\n",
        "\n",
        "C. Can I get higher accuracy with additional hyperparameters (higher than 80.0)?\n",
        "\n",
        "In my experiment, the highest accuracy obtained was 80% using a learning rate of 0.05, 20 epochs, and 5% of the training data. It is possible that higher accuracy can be obtained by using additional hyperparameters or by tuning the current hyperparameters. However, in my experiment, I did not observe any hyperparameter settings that yielded an accuracy higher than 80%.\n",
        "\n",
        "D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "In my experiment, I found that training for more epochs did not necessarily improve the accuracy of the model. In fact, using a lower number of epochs (5 or 10) sometimes yielded higher accuracy compared to using a higher number of epochs (20 or 50). Therefore, it is not always worth training for more epochs while keeping all other hyperparameters fixed. It is important to find the sweet spot where the model has learned enough to make accurate predictions, without overfitting to the training data."
      ],
      "metadata": {
        "id": "38rA_Kp3wiBX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}